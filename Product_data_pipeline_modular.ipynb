{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from socket import timeout\n",
    "import time\n",
    "from time import sleep\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('max_colwidth',500)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    url (string):  a url from an Indeed.com search.\n",
    "    \n",
    "    This represents step 1 of searching, clicking through, and scraping job descriptions\n",
    "    on Indeed.com.\n",
    "    \n",
    "    Returns:\n",
    "    ids (list): a list of link IDs so we can click through to the actual job postings.\n",
    "    \"\"\"\n",
    "    html = urlopen(url)\n",
    "    \n",
    "    site = html.read()\n",
    "    soup = Soup(site, 'html.parser')\n",
    "    results = soup.find(id = 'resultsCol')\n",
    "    \n",
    "    page_urls = [link.get('href') for link in results.find_all('a')]\n",
    "    page_urls = [link for link in page_urls if '/rc/' in \n",
    "          str(link)]\n",
    "    \n",
    "    ids = []\n",
    "    for link in page_urls:\n",
    "        start = link.find('jk=') + 3\n",
    "        end = link.find('&fccid=')\n",
    "        ids.append(link[start:end])\n",
    "    return(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_dict(url):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    url (string): a component page url from the results of an Indeed.com search.\n",
    "    \n",
    "    Parses the html and extracts key information.\n",
    "    \n",
    "    Returns:\n",
    "    html_content_dict (dict): a dictionary of job info.\n",
    "    \n",
    "    Success key indicates whether the site format is wonky and should be tossed or \n",
    "    parsed differently later. Url key holds the string of the site url. Job title,\n",
    "    company, location, and job descriptions are extracted from the job page html.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # In case website times out\n",
    "    backoff=15\n",
    "    \n",
    "    nulldict = {\"success\": False,\n",
    "                'url': url,\n",
    "                'description': \"\",\n",
    "                'title': \"\",\n",
    "                'company': \"\",\n",
    "                'location': \"\"}\n",
    "    \n",
    "    succeeded = False\n",
    "    for tries in range(3):\n",
    "        if succeeded == False:\n",
    "            try:\n",
    "                html = urlopen(url)\n",
    "                succeeded = True\n",
    "            # In case server needs another try\n",
    "            except timeout:\n",
    "                sleep(backoff)\n",
    "            # In case server returns 500, move on\n",
    "            except urllib.error.HTTPError():\n",
    "                return nulldict\n",
    "    \n",
    "    if succeeded == False:\n",
    "        return nulldict\n",
    "    \n",
    "    site = html.read()\n",
    "    soup = Soup(site, 'html.parser')\n",
    "    \n",
    "    for i in soup.find_all(\"script\"):\n",
    "        i.decompose()\n",
    "    for i in soup.find_all(\"style\"):\n",
    "        i.decompose()\n",
    "    for i in soup.find_all(\"noscript\"):\n",
    "        i.decompose()\n",
    "    for i in soup.find_all(\"meta\"):\n",
    "        i.decompose()\n",
    "    \n",
    "    #for updates as the script runs, uncomment below\n",
    "    #print(url)\n",
    "    \n",
    "    #look at the soup, figure out the right call to get text, title, company, location\n",
    "    tag = soup.find(\"div\", {\"class\": \"jobsearch-JobComponent-description\"})\n",
    "    if tag is None:\n",
    "        return {\"success\": False, \"url\": url, 'urlresponse': html, \"urltext\" : site, \"urlsoup\" : soup}\n",
    "    text = soup.find(\"div\", {\"class\": \"jobsearch-JobComponent-description\"}).get_text()\n",
    "    lines = (line.strip() for line in text.splitlines()) \n",
    "    lines = [l for l in lines if l != '']\n",
    "    cleaned = ' '.join(lines)\n",
    "    \n",
    "    title = soup.find('title').get_text().split(sep = \" -\")[0]\n",
    "    company = soup.find('div',{'class':'jobsearch-InlineCompanyRating'}).find('div').get_text()\n",
    "    location = soup.find('div',{'class':'jobsearch-InlineCompanyRating'}).findAll('div')[-1].get_text()\n",
    "    \n",
    "    html_content_dict = {\"success\": True,\n",
    "                        'url': url,\n",
    "                        'description':cleaned,\n",
    "                        'title': title,\n",
    "                        'company': company,\n",
    "                        'location': location}\n",
    "    \n",
    "    return html_content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_descriptions(search_url, total_job_count, offset):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    search_url (string): url for Indeed.com search.\n",
    "    total_job_count (int): total number of jobs to be retrieved\n",
    "    offset (int): to break task into smaller pieces, start later in search\n",
    "\n",
    "    Gets job IDs from that search page.\n",
    "    Returns list of dicts containing job content\n",
    "    using the helper functions above.\n",
    "    Each job description is a dict.\n",
    "    \n",
    "    Returns:\n",
    "    descriptions (list): list of dicts containing job content\n",
    "    \"\"\"\n",
    "    # how many jobs from this search?\n",
    "    count = total_job_count\n",
    "    \n",
    "    # In case website times out\n",
    "    backoff=15\n",
    "    succeeded = False\n",
    "    for tries in range(3):\n",
    "        if succeeded == False:\n",
    "            try:\n",
    "                html = urlopen(search_url)\n",
    "                succeeded = True\n",
    "            except timeout:\n",
    "                sleep(backoff)\n",
    "        \n",
    "    site = html.read()\n",
    "    soup = Soup(site, 'html.parser')\n",
    "    results = soup.find(id = 'resultsCol')\n",
    "    divs = results.find_all('div')\n",
    "    \n",
    "    print(\"Getting job ids...\")\n",
    "    i = 0 \n",
    "    job_ids = []\n",
    "    while i < count:\n",
    "        url = search_url + '&start=' + str(i + offset)\n",
    "        job_ids = job_ids + get_links(url)\n",
    "        sleep(1)\n",
    "        i = i + 10\n",
    "    print(\"Finished pulling job ids. Getting descriptions...\")\n",
    "    \n",
    "    descriptions = []\n",
    "    \n",
    "    #creating a counter to keep track of progress\n",
    "    iteration_counter = 0\n",
    "    \n",
    "    for id in job_ids:\n",
    "        #Printing out progress mod 10\n",
    "        if iteration_counter % 10 == 0:\n",
    "            print(\"Grabbing job \" + str(iteration_counter))\n",
    "        iteration_counter = iteration_counter + 1\n",
    "        url = \"https://www.indeed.com/viewjob?jk=\" + str(id)\n",
    "        descriptions.append(get_page_dict(url))\n",
    "        sleep(1)\n",
    "    \n",
    "    print(\"Finished grabbing jobs.\")\n",
    "    return(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_data_collection(radius_int, fromage_int, sort_type, total_job_count, offset):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    radius_int (int): radius of geographic search. Furthest reaches of NYC are 15-20 miles from midtown.\n",
    "    fromage_int (int): \"from age\" a limit on days old the search results can be\n",
    "    sort_type (string): Indeed.com allows search by recommended or date. Date preferred here.\n",
    "    format(jobtitletext = searchwords)\n",
    "    offset (int): offset for starting later in search results for smaller tasks\n",
    "    \n",
    "    This function formats the search string appropriately and runs the scraping task.\n",
    "    \n",
    "    Returns: \n",
    "    scraped_content (list): list of dicts where each dict contains info from one job posting.\n",
    "    \n",
    "    \"\"\"\n",
    "    urlstring = \"https://www.indeed.com/jobs?radius={radius}&l=New+York%2C+NY&fromage={fromage}&sort={sort}\"\n",
    "    formatted_string = urlstring.format(radius = radius_int, fromage = fromage_int, sort = sort_type)\n",
    "    \n",
    "    scraped_content = get_job_descriptions(formatted_string, total_job_count, offset)\n",
    "    \n",
    "    return scraped_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_scraped_data(scraped_data_list):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    scraped_data_list (list): list of dicts, each of which contains a job.\n",
    "    \n",
    "    Filters on the success of the parse (Indeed.com has two different html formats).\n",
    "    convert list of dicts into a dataframe.\n",
    "    \n",
    "    Return:\n",
    "    job_df (pd dataframe): dataframe where each row is a job description\n",
    "    \"\"\"\n",
    "    #filter on success, convert list of identical dicts into pandas df\n",
    "    filtered_data = [x for x in scraped_data_list if x['success']]\n",
    "    job_df = pd.DataFrame(filtered_data)\n",
    "    \n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(offset, goal_job_desc_count, step):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    offset (int): start scraping here. For breaking task into chunks\n",
    "    goal_job_desc_count (int): how many total job descriptions required\n",
    "    step (int): scraping batched in groups of this many. \n",
    "    \n",
    "    Because these scraping tasks can produce unexpected errors,\n",
    "    scraping is batched and can be picked up based on last \n",
    "    reported signpost quantity.\n",
    "    \n",
    "    Returns:\n",
    "    Saves each batch to labeled csv in working directory.\n",
    "    \"\"\"    \n",
    "    for i in range(offset, goal_job_desc_count, step):\n",
    "        scraped_data =  run_data_collection(15,15,\"date\", 500, i)\n",
    "        cleaned_data = clean_scraped_data(scraped_data)\n",
    "        cleaned_data.to_csv(\"indeed_jobs_{}_overnight.csv\".format(i))\n",
    "        print(\"finished \"+str(i+500)+\" jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape 10,000 job descriptions in batches of 500 starting at record 0 from the search\n",
    "\n",
    "scrape_data(0,10000,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
